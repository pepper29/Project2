{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ (streamlit, kobert)"
      ],
      "metadata": {
        "id": "-JwoFcqL9FBc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qONxoJQ18iuJ",
        "outputId": "b11c66c7-8f85-4aab-b5ba-2246f53bab5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: streamlit-folium in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from streamlit-folium) (2.11.3)\n",
            "Requirement already satisfied: streamlit>=1.2 in /usr/local/lib/python3.8/dist-packages (from streamlit-folium) (1.18.1)\n",
            "Requirement already satisfied: branca in /usr/local/lib/python3.8/dist-packages (from streamlit-folium) (0.6.0)\n",
            "Requirement already satisfied: folium>=0.13 in /usr/local/lib/python3.8/dist-packages (from streamlit-folium) (0.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from folium>=0.13->streamlit-folium) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from folium>=0.13->streamlit-folium) (1.21.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->streamlit-folium) (2.0.1)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (2.13.0)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (9.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (6.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (6.0.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (0.10.2)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (23.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (7.1.2)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.0.1)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (2.2.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (5.3.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (2.8.2)\n",
            "Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.5)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (3.1.30)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (7.1.2)\n",
            "Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (0.20.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.5.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (13.3.1)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.3.5)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (3.19.6)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (4.4.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (0.8.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=1.2->streamlit-folium) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=1.2->streamlit-folium) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=1.2->streamlit-folium) (0.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython!=3.1.19->streamlit>=1.2->streamlit-folium) (4.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.4->streamlit>=1.2->streamlit-folium) (3.12.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25->streamlit>=1.2->streamlit-folium) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->streamlit>=1.2->streamlit-folium) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (4.0.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit>=1.2->streamlit-folium) (2.1.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.14.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit>=1.2->streamlit-folium) (2.14.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators>=0.2->streamlit>=1.2->streamlit-folium) (4.4.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit>=1.2->streamlit-folium) (5.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=1.2->streamlit-folium) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=1.2->streamlit-folium) (22.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=1.2->streamlit-folium) (5.10.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=10.11.0->streamlit>=1.2->streamlit-folium) (0.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.8/dist-packages (5.2.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok) (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.8/dist-packages (1.7.0.post2)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (1.21.6)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.8/dist-packages (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (0.29.33)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (23.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.96)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.0.12->transformers) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-w8twrw9e\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-w8twrw9e\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: boto3<=1.15.18 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.15.18)\n",
            "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.7.0.post2)\n",
            "Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.8.0)\n",
            "Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.1.96)\n",
            "Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.10.1)\n",
            "Requirement already satisfied: transformers<=4.8.1,>=4.8.1 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (23.1.21)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n",
            "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (1.18.18)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (0.3.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (0.10.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.33)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.0)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.25.1)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.53)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.12)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit -q\n",
        "!pip install streamlit-folium\n",
        "!pip install pyngrok\n",
        "\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# drive mount"
      ],
      "metadata": {
        "id": "YMbIaiSiNkz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#êµ¬ê¸€ë“œë¼ì´ë¸Œ ì—°ë™\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jgtilUKhgQn",
        "outputId": "4ae697f5-a68e-4b1b-b61d-64ce7b0816a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ngrok ê°œì¸ í† í° ê°€ì ¸ì˜¤ê¸°\n",
        "https://ngrok.com/"
      ],
      "metadata": {
        "id": "rTLjZJ-JNVu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token('2LOn2Tf27j9xWtReXqYazgvzxJl_4dXNUWYG75KmpLCLnEVo7')"
      ],
      "metadata": {
        "id": "XZ766ErF9i8i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# py íŒŒì¼ ë§Œë“¤ê¸° (stremlit ì—°ê²°)\n",
        "py ì €ì¥í•˜ê³  streamlit ì›¹ì—ì„œ reload\n",
        "\n",
        "ì½”ë“œ ìˆ˜ì •í• ë–„ : *%%writefile app.py* --> ì£¼ì„ì²˜ë¦¬ í•´ì•¼ ì½”ë“œë§ˆë‹¤ ìƒ‰ê¹” ë³´ì„"
      ],
      "metadata": {
        "id": "kn285XHLNnh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile app2.py\n",
        "\n",
        "import streamlit as st\n",
        "import streamlit.components.v1 as html\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import folium\n",
        "from folium.plugins import MiniMap\n",
        "from streamlit_folium import st_folium\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "# kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "# í˜ì´ì§€ì˜ ê¸°ë³¸ ì„¤ì • êµ¬ì„±\n",
        "st.set_page_config(\n",
        " layout=\"wide\",\n",
        " page_title='ì˜¤ëŠ˜ ì´ê±° ë¨¹ì–´')\n",
        "\n",
        "#######################################################################################################\n",
        "#### ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ####\n",
        "\n",
        "# device = torch.device(\"cuda:0\") #GPUì‚¬ìš©\n",
        "device = torch.device(\"cpu\")  #CPUì‚¬ìš©\n",
        "\n",
        "bertmodel, vocab = get_pytorch_kobert_model()\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 20\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=17,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)\n",
        "\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0} ]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "def softmax(vals, idx):\n",
        "    valscpu = vals.detach().cpu().squeeze(0)\n",
        "    a = 0\n",
        "    for i in valscpu:\n",
        "        a += np.exp(i)\n",
        "    return ((np.exp(valscpu[idx]))/a).item() * 100\n",
        "\n",
        "\n",
        "def testModel(model, seq):\n",
        "    cate = [\"ê³±ì°½\",\"êµ­ìˆ˜\",\"ëˆì¹´ì¸ \", \"ë””ì €íŠ¸\",\"ë¼ë©˜\",\"ë²„ê±°\", \"ë² ì´ì»¤ë¦¬\", \"ë¶„ì‹\", \"ìŠ¤ì‹œ\", \"ì•„ì‹œì•„ìŒì‹\", \"ì–‘ì‹\", \"ì „ê³¨\", \"ì¤‘ì‹\", \"ì¹˜í‚¨\", \"íƒ€ì½”\", \"í•œì‹\", \"í•´ì‚°ë¬¼\"]\n",
        "    tmp = [seq]\n",
        "    transform = nlp.data.BERTSentenceTransform(tok, max_len, pad=True, pair=False)\n",
        "    tokenized = transform(tmp)\n",
        "\n",
        "    modelload.eval()\n",
        "    result = modelload(torch.tensor([tokenized[0]]).to(device), [tokenized[1]], torch.tensor(tokenized[2]).to(device)) \n",
        "    idx = result.argmax().cpu().item() \n",
        "    result2 = F.softmax(result, dim=1).sort() \n",
        "\n",
        "    return cate[result2[1][0][-1]],round((result2[0][0][-1]).item(), 4)*100, cate[result2[1][0][-2]],round((result2[0][0][-2]).item(), 4)*100, cate[result2[1][0][-3]],round((result2[0][0][-3]).item(), 4)*100\n",
        "\n",
        "# ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í•œë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œì— ì €ì¥í•˜ê¸°\n",
        "@st.cache_resource\n",
        "def cache_model(path, modelname):\n",
        "    modelload = torch.load(\"/content/drive/MyDrive/final project/model/model6.pt\", map_location=torch.device('cpu')) # cpuì‚¬ìš©ì‹œ\n",
        "    # modelload = torch.load(\"/content/drive/MyDrive/final project/model/model6.pt\") # gpuì‚¬ìš©ì‹œ\n",
        "    modelload.eval()\n",
        "    return modelload\n",
        "\n",
        "modelload = cache_model('/content/drive/MyDrive/final project/model/','model6.pt')\n",
        "\n",
        "# ì¹´ì¹´ì˜¤ api\n",
        "@st.cache_resource\n",
        "def elec_location(region,page_num):\n",
        "    url = 'https://dapi.kakao.com/v2/local/search/keyword.json'\n",
        "    params = {'query': region,'page': page_num, 'sort' : 'popularity'}\n",
        "    headers = {\"Authorization\": \"KakaoAK 6dd31dbd3f7b90aed3f5591fdde29527\"}\n",
        "\n",
        "    places = requests.get(url, params=params, headers=headers).json()['documents']\n",
        "\n",
        "    return places\n",
        "\n",
        "def elec_info(places):\n",
        "    X = []\n",
        "    Y = []\n",
        "    stores = []\n",
        "    road_address = []\n",
        "    phone = []\n",
        "    place_url = []\n",
        "    ID = []\n",
        "    for place in places:\n",
        "        X.append(float(place['x']))\n",
        "        Y.append(float(place['y']))\n",
        "        stores.append(place['place_name'])\n",
        "        road_address.append(place['road_address_name'])\n",
        "        phone.append(place['phone'])\n",
        "        place_url.append(place['place_url'])\n",
        "        ID.append(place['id'])\n",
        "\n",
        "    ar = np.array([ID,stores, X, Y, road_address, phone, place_url]).T\n",
        "    df = pd.DataFrame(ar, columns = ['ID','stores', 'X', 'Y','road_address','phone','place_url'])\n",
        "    return df\n",
        "\n",
        "def keywords(location_name):\n",
        "    df = None\n",
        "    page_num = int(1)\n",
        "    for loca in location_name:\n",
        "        for page in range(1,page_num+1):\n",
        "            local_name = elec_location(loca, page)\n",
        "            local_elec_info = elec_info(local_name)\n",
        "\n",
        "            if df is None:\n",
        "                df = local_elec_info\n",
        "            elif local_elec_info is None:\n",
        "                continue\n",
        "            else:\n",
        "                df = pd.concat([df, local_elec_info],join='outer', ignore_index = True)\n",
        "    return df\n",
        "\n",
        "def make_map(dfs, m):\n",
        "    \n",
        "    minimap = MiniMap() \n",
        "    m.add_child(minimap)\n",
        "\n",
        "    for i in range(len(dfs)):\n",
        "        folium.Marker([dfs['Y'][i],dfs['X'][i]],\n",
        "                      tooltip=dfs['stores'][i],\n",
        "                      popup = '<iframe width=\"800\" height=\"400\" src=\"' + df['place_url'][i] + '\"title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay;  clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>',\n",
        "                      ).add_to(m)\n",
        "    return m\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "tab1, tab2 = st.tabs(['search', 'map'])\n",
        "\n",
        "# user ì…ë ¥ê°’ ì €ì¥\n",
        "if 'user_input' not in st.session_state:\n",
        "    st.session_state['user_input'] = ''\n",
        "\n",
        "if 'user_location_input' not in st.session_state:\n",
        "    st.session_state['user_location_input'] = ''\n",
        "\n",
        "with tab1:\n",
        "    st.subheader('ğŸ’­ì˜¤ëŠ˜ë„ ë¬´ì—‡ì„ ë¨¹ì„ì§€ ê³ ë¯¼í•˜ê³  ê³„ì‹ ê°€ìš”?')\n",
        "\n",
        "    value = st.text_area('ì§€ê¸ˆ ìƒê°ë‚˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  Ctrl+Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!', placeholder = 'Ex) ìœ¡ì¦™ì´ íŒ¡íŒ¡ í„°ì§€ëŠ” ê³ ì†Œí•œ ìŒì‹ì´ ë¨¹ê³ ì‹¶ì–´.', key='user_input')\n",
        "    cat1,val1, cat2,val2, cat3,val3 = testModel(model ,st.session_state.user_input)\n",
        "\n",
        "    if value:\n",
        "       st.header(cat1, 'ì´ ìŒì‹ì€ ì–´ë– ì‹ ê°€ìš”?')\n",
        "\n",
        "       st.subheader(f\"{cat1}ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” {round(val1, 2)}% ì…ë‹ˆë‹¤.\")\n",
        "\n",
        "       st.write('ì…ë ¥ë¬¸ì¥ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ìŒì‹ TOP3 ì…ë‹ˆë‹¤.')\n",
        "       st.write('ğŸ¥‡',cat1, 'ì‹ ë¢°ë„ëŠ”', round(val1, 2),'% ì…ë‹ˆë‹¤.')\n",
        "       st.write('ğŸ¥ˆ',cat2, 'ì‹ ë¢°ë„ëŠ”', round(val2, 2),'% ì…ë‹ˆë‹¤.')\n",
        "       st.write('ğŸ¥‰',cat3, 'ì‹ ë¢°ë„ëŠ”', round(val3, 2),'% ì…ë‹ˆë‹¤.')\n",
        "\n",
        "\n",
        "with tab2:\n",
        "\n",
        "    st.subheader('ğŸš‡ê°€ì‹œë ¤ëŠ” ì§€ì—­ì´ ì–´ë””ì¸ê°€ìš”?')\n",
        "    location = st.text_input('ì§€í•˜ì² ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒì‹ì ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”! ex) ê°•ë‚¨ì—­', key='user_location_input')\n",
        "    user_location = st.session_state.user_location_input\n",
        "\n",
        "    # ì»¬ëŸ¼ì„ ë‘ ê°œë¡œ ë‚˜ëˆ„ê¸°\n",
        "    left_column, right_column = st.columns(2)\n",
        "\n",
        "    with left_column:\n",
        "        if location:\n",
        "            kakao_location = [user_location + ' ' + cat1]\n",
        "            try:\n",
        "              df = keywords(kakao_location)\n",
        "              lat = 0\n",
        "              lon = 0\n",
        "              for i in df['Y']:\n",
        "                  lat += float(i)\n",
        "              for j in df['X']:\n",
        "                  lon += float(j)\n",
        "              lat = lat/len(df['Y'])\n",
        "              lon = lon/len(df['X'])\n",
        "              m = folium.Map(kakao_location=[lat, lon],   # ê¸°ì¤€ì¢Œí‘œ: current_location\n",
        "                            zoom_start=16)\n",
        "              make_map = make_map(df, m)\n",
        "              st_folium(make_map, width = 1000, height = 500, zoom=14, center = [lat, lon])\n",
        "              df = df.drop(columns = ['ID', 'X', 'Y'])\n",
        "              with right_column:\n",
        "                  st.dataframe(df)\n",
        "                  st.write('ê²°ê³¼ëŠ” ì¸ê¸°ë„ìˆœìœ¼ë¡œ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')\n",
        "            except:\n",
        "              st.write('ì•„ì‰½ê²Œë„ ' + user_location + ' ê·¼ì²˜ì—ëŠ” ' + cat1 + ' ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤ã… ã… ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcbSRexMgUAg",
        "outputId": "71436d82-ee4d-4b8d-80c3-9773f212e3c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat app2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7swO3uYtHgY",
        "outputId": "7fcf1ecc-bfaf-4b88-ed7f-33e7e33008c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "import streamlit as st\n",
            "import streamlit.components.v1 as html\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import requests\n",
            "import folium\n",
            "from folium.plugins import MiniMap\n",
            "from streamlit_folium import st_folium\n",
            "\n",
            "# torch\n",
            "import torch\n",
            "from torch import nn\n",
            "import torch.nn.functional as F\n",
            "import torch.optim as optim\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "import gluonnlp as nlp\n",
            "import numpy as np\n",
            "from tqdm import tqdm, tqdm_notebook\n",
            "\n",
            "# kobert\n",
            "from kobert.utils import get_tokenizer\n",
            "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
            "\n",
            "#transformers\n",
            "from transformers import AdamW\n",
            "from transformers.optimization import get_cosine_schedule_with_warmup\n",
            "\n",
            "# í˜ì´ì§€ì˜ ê¸°ë³¸ ì„¤ì • êµ¬ì„±\n",
            "st.set_page_config(\n",
            " layout=\"wide\",\n",
            " page_title='ì˜¤ëŠ˜ ì´ê±° ë¨¹ì–´')\n",
            "\n",
            "# ë°°ê²½í™”ë©´ ì„¤ì •\n",
            "# def add_bg_from_url():\n",
            "#     st.markdown(\n",
            "#          f\"\"\"\n",
            "#          <style>\n",
            "#          .stApp {{\n",
            "#              background-image: url(\"https://i.pinimg.com/564x/30/ed/e7/30ede74766f91c06e51f920b40a4cafb.jpg\");\n",
            "#              background-attachment: fixed;\n",
            "#              background-size: cover\n",
            "#          }}\n",
            "#          </style>\n",
            "#          \"\"\",\n",
            "#          unsafe_allow_html=True\n",
            "#      )\n",
            "\n",
            "# add_bg_from_url()\n",
            "\n",
            "#######################################################################################################\n",
            "#### ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ####\n",
            "\n",
            "# device = torch.device(\"cuda:0\") #GPUì‚¬ìš©\n",
            "device = torch.device(\"cpu\")  #CPUì‚¬ìš©\n",
            "\n",
            "bertmodel, vocab = get_pytorch_kobert_model()\n",
            "\n",
            "tokenizer = get_tokenizer()\n",
            "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
            "\n",
            "class BERTDataset(Dataset):\n",
            "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
            "        transform = nlp.data.BERTSentenceTransform(\n",
            "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
            "\n",
            "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
            "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
            "\n",
            "    def __getitem__(self, i):\n",
            "        return (self.sentences[i] + (self.labels[i], ))\n",
            "\n",
            "    def __len__(self):\n",
            "        return (len(self.labels))\n",
            "\n",
            "max_len = 64\n",
            "batch_size = 64\n",
            "warmup_ratio = 0.1\n",
            "num_epochs = 20\n",
            "max_grad_norm = 1\n",
            "log_interval = 200\n",
            "learning_rate =  5e-5\n",
            "\n",
            "class BERTClassifier(nn.Module):\n",
            "    def __init__(self,\n",
            "                 bert,\n",
            "                 hidden_size = 768,\n",
            "                 num_classes=17,\n",
            "                 dr_rate=None,\n",
            "                 params=None):\n",
            "        super(BERTClassifier, self).__init__()\n",
            "        self.bert = bert\n",
            "        self.dr_rate = dr_rate\n",
            "                 \n",
            "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
            "        if dr_rate:\n",
            "            self.dropout = nn.Dropout(p=dr_rate)\n",
            "    \n",
            "    def gen_attention_mask(self, token_ids, valid_length):\n",
            "\n",
            "        attention_mask = torch.zeros_like(token_ids)\n",
            "        for i, v in enumerate(valid_length):\n",
            "            attention_mask[i][:v] = 1\n",
            "        return attention_mask.float()\n",
            "\n",
            "    def forward(self, token_ids, valid_length, segment_ids):\n",
            "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
            "\n",
            "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)\n",
            "\n",
            "        if self.dr_rate:\n",
            "            out = self.dropout(pooler)\n",
            "        return self.classifier(out)\n",
            "\n",
            "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
            "\n",
            "no_decay = ['bias', 'LayerNorm.weight']\n",
            "optimizer_grouped_parameters = [\n",
            "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
            "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0} ]\n",
            "\n",
            "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "\n",
            "def calc_accuracy(X,Y):\n",
            "    max_vals, max_indices = torch.max(X, 1)\n",
            "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
            "    return train_acc\n",
            "\n",
            "def softmax(vals, idx):\n",
            "    valscpu = vals.detach().cpu().squeeze(0)\n",
            "    a = 0\n",
            "    for i in valscpu:\n",
            "        a += np.exp(i)\n",
            "    return ((np.exp(valscpu[idx]))/a).item() * 100\n",
            "\n",
            "\n",
            "def testModel(model, seq):\n",
            "    cate = [\"ê³±ì°½\",\"êµ­ìˆ˜\",\"ëˆì¹´ì¸ \", \"ë””ì €íŠ¸\",\"ë¼ë©˜\",\"ë²„ê±°\", \"ë² ì´ì»¤ë¦¬\", \"ë¶„ì‹\", \"ìŠ¤ì‹œ\", \"ì•„ì‹œì•„ìŒì‹\", \"ì–‘ì‹\", \"ì „ê³¨\", \"ì¤‘ì‹\", \"ì¹˜í‚¨\", \"íƒ€ì½”\", \"í•œì‹\", \"í•´ì‚°ë¬¼\"]\n",
            "    tmp = [seq]\n",
            "    transform = nlp.data.BERTSentenceTransform(tok, max_len, pad=True, pair=False)\n",
            "    tokenized = transform(tmp)\n",
            "\n",
            "    modelload.eval()\n",
            "    result = modelload(torch.tensor([tokenized[0]]).to(device), [tokenized[1]], torch.tensor(tokenized[2]).to(device)) \n",
            "    idx = result.argmax().cpu().item() #ì¶œë ¥ì˜ ìµœëŒ€ê°’ì´ ë‚˜ì˜¤ê²Œí•¨\n",
            "    result2 = F.softmax(result, dim=1).sort() #ê° ê°’ì— ëŒ€í•œ softmaxí•¨ìˆ˜ ì ìš©\n",
            "\n",
            "    #return cate[idx], softmax(result,idx)\n",
            "    return cate[result2[1][0][-1]],round((result2[0][0][-1]).item(), 4)*100, cate[result2[1][0][-2]],round((result2[0][0][-2]).item(), 4)*100, cate[result2[1][0][-3]],round((result2[0][0][-3]).item(), 4)*100\n",
            "\n",
            "# ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í•œë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œì— ì €ì¥í•˜ê¸°\n",
            "@st.cache_resource\n",
            "def cache_model(path, modelname):\n",
            "    modelload = torch.load(\"/content/drive/MyDrive/final project/model/model6.pt\", map_location=torch.device('cpu')) # cpuì‚¬ìš©ì‹œ\n",
            "    # modelload = torch.load(\"/content/drive/MyDrive/final project/model/model6.pt\") # gpuì‚¬ìš©ì‹œ\n",
            "    modelload.eval()\n",
            "    return modelload\n",
            "\n",
            "modelload = cache_model('/content/drive/MyDrive/final project/model/','model6.pt')\n",
            "\n",
            "# ì¹´ì¹´ì˜¤ api\n",
            "@st.cache_resource\n",
            "def elec_location(region,page_num):\n",
            "    url = 'https://dapi.kakao.com/v2/local/search/keyword.json'\n",
            "    params = {'query': region,'page': page_num, 'sort' : 'popularity'}\n",
            "    headers = {\"Authorization\": \"KakaoAK 6dd31dbd3f7b90aed3f5591fdde29527\"}\n",
            "\n",
            "    places = requests.get(url, params=params, headers=headers).json()['documents']\n",
            "\n",
            "    return places\n",
            "\n",
            "def elec_info(places):\n",
            "    X = []\n",
            "    Y = []\n",
            "    stores = []\n",
            "    road_address = []\n",
            "    phone = []\n",
            "    place_url = []\n",
            "    ID = []\n",
            "    for place in places:\n",
            "        X.append(float(place['x']))\n",
            "        Y.append(float(place['y']))\n",
            "        stores.append(place['place_name'])\n",
            "        road_address.append(place['road_address_name'])\n",
            "        phone.append(place['phone'])\n",
            "        place_url.append(place['place_url'])\n",
            "        ID.append(place['id'])\n",
            "\n",
            "    ar = np.array([ID,stores, X, Y, road_address, phone, place_url]).T\n",
            "    df = pd.DataFrame(ar, columns = ['ID','stores', 'X', 'Y','road_address','phone','place_url'])\n",
            "    return df\n",
            "\n",
            "def keywords(location_name):\n",
            "    df = None\n",
            "    page_num = int(1)\n",
            "    for loca in location_name:\n",
            "        for page in range(1,page_num+1):\n",
            "            local_name = elec_location(loca, page)\n",
            "            local_elec_info = elec_info(local_name)\n",
            "\n",
            "            if df is None:\n",
            "                df = local_elec_info\n",
            "            elif local_elec_info is None:\n",
            "                continue\n",
            "            else:\n",
            "                df = pd.concat([df, local_elec_info],join='outer', ignore_index = True)\n",
            "    return df\n",
            "\n",
            "def make_map(dfs, m):\n",
            "    \n",
            "    minimap = MiniMap() \n",
            "    m.add_child(minimap)\n",
            "\n",
            "    for i in range(len(dfs)):\n",
            "        folium.Marker([dfs['Y'][i],dfs['X'][i]],\n",
            "                      tooltip=dfs['stores'][i],\n",
            "                      popup = '<iframe width=\"800\" height=\"400\" src=\"' + df['place_url'][i] + '\"title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay;  clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>',\n",
            "                      ).add_to(m)\n",
            "    return m\n",
            "\n",
            "#######################################################################################################\n",
            "\n",
            "tab1, tab2 = st.tabs(['search', 'map'])\n",
            "\n",
            "# user ì…ë ¥ê°’ ì €ì¥\n",
            "if 'user_input' not in st.session_state:\n",
            "    st.session_state['user_input'] = ''\n",
            "\n",
            "if 'user_location_input' not in st.session_state:\n",
            "    st.session_state['user_location_input'] = ''\n",
            "\n",
            "with tab1:\n",
            "    st.subheader('ğŸ’­ì˜¤ëŠ˜ë„ ë¬´ì—‡ì„ ë¨¹ì„ì§€ ê³ ë¯¼í•˜ê³  ê³„ì‹ ê°€ìš”?')\n",
            "\n",
            "    value = st.text_area('ì§€ê¸ˆ ìƒê°ë‚˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  Ctrl+Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!', placeholder = 'Ex) ìœ¡ì¦™ì´ íŒ¡íŒ¡ í„°ì§€ëŠ” ê³ ì†Œí•œ ìŒì‹ì´ ë¨¹ê³ ì‹¶ì–´.', key='user_input')\n",
            "    cat1,val1, cat2,val2, cat3,val3 = testModel(model ,st.session_state.user_input)\n",
            "\n",
            "    if value:\n",
            "       st.header(cat1, 'ì´ ìŒì‹ì€ ì–´ë– ì‹ ê°€ìš”?')\n",
            "\n",
            "       st.subheader(f\"{cat1}ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” {round(val1, 2)}% ì…ë‹ˆë‹¤.\")\n",
            "       #st.write(cat1, 'ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤.', 'ì‹ ë¢°ë„ëŠ”', round(val1, 2), '% ì…ë‹ˆë‹¤.')\n",
            "\n",
            "       st.write('ì…ë ¥ë¬¸ì¥ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ìŒì‹ TOP3 ì…ë‹ˆë‹¤.')\n",
            "       st.write('ğŸ¥‡',cat1, 'ì‹ ë¢°ë„ëŠ”', round(val1, 2),'% ì…ë‹ˆë‹¤.')\n",
            "       st.write('ğŸ¥ˆ',cat2, 'ì‹ ë¢°ë„ëŠ”', round(val2, 2),'% ì…ë‹ˆë‹¤.')\n",
            "       st.write('ğŸ¥‰',cat3, 'ì‹ ë¢°ë„ëŠ”', round(val3, 2),'% ì…ë‹ˆë‹¤.')\n",
            "\n",
            "\n",
            "with tab2:\n",
            "\n",
            "    # ì»¬ëŸ¼ì„ ë‘ ê°œë¡œ ë‚˜ëˆ„ê¸°\n",
            "    left_column, right_column = st.columns(2)\n",
            "\n",
            "    st.subheader('ğŸš‡ê°€ì‹œë ¤ëŠ” ì§€ì—­ì´ ì–´ë””ì¸ê°€ìš”?')\n",
            "    location = st.text_input('ì§€í•˜ì² ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒì‹ì ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”! ex) ê°•ë‚¨ì—­', key='user_location_input')\n",
            "    user_location = st.session_state.user_location_input\n",
            "\n",
            "    with left_column:\n",
            "        if location:\n",
            "            kakao_location = [user_location + ' ' + cat1]\n",
            "            try:\n",
            "              df = keywords(kakao_location)\n",
            "              lat = 0\n",
            "              lon = 0\n",
            "              for i in df['Y']:\n",
            "                  lat += float(i)\n",
            "              for j in df['X']:\n",
            "                  lon += float(j)\n",
            "              lat = lat/len(df['Y'])\n",
            "              lon = lon/len(df['X'])\n",
            "              m = folium.Map(kakao_location=[lat, lon],   # ê¸°ì¤€ì¢Œí‘œ: current_location\n",
            "                            zoom_start=16)\n",
            "              make_map = make_map(df, m)\n",
            "              st_folium(make_map, width = 1000, height = 500, zoom=16, center = [lat, lon])\n",
            "              df = df.drop(columns = ['ID', 'X', 'Y'])\n",
            "              with right_column:\n",
            "                  st.dataframe(df)\n",
            "                  st.write('ê²°ê³¼ëŠ” ì¸ê¸°ë„ìˆœìœ¼ë¡œ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')\n",
            "                except:\n",
            "                  st.write('ì•„ì‰½ê²Œë„ ' + user_location + ' ê·¼ì²˜ì—ëŠ” ' + cat1 + ' ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤ã… ã… ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# streamlit run"
      ],
      "metadata": {
        "id": "CrQjfYxwN8KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app2.py --server.port 80 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzNZCMGBJxkQ",
        "outputId": "9410d0ee-306b-4dc5-b6f4-ba51d726d0af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ngrok ì—°ê²°í•´ì„œ ì£¼ì†Œ ë°›ê¸°"
      ],
      "metadata": {
        "id": "Qz34p3i1OA3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = ngrok.connect(port='80')\n",
        "url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ_LI3eNJ5Eo",
        "outputId": "da8043e3-00f9-4077-9d70-aff1cd9bec87"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://3c12-35-224-107-147.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ìƒˆ ì£¼ì†Œ ë°›ì„ë•Œ kill í•˜ê¸°"
      ],
      "metadata": {
        "id": "3SSkgD37OHm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "h9ZrUmTJKHhv"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}